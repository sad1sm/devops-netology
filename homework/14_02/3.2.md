# Домашнее задание к занятию «Установка Kubernetes»

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развёрнутые ВМ с ОС Ubuntu 20.04-lts.


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).
2. [Документация kubespray](https://kubespray.io/).

-----

### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды.
2. В качестве CRI — containerd.
3. Запуск etcd производить на мастере.
4. Способ установки выбрать самостоятельно.

---
### Ответ:
Я выбрал установку с помощью `kubeadm`.  
На всех нодах выполнил следующие команды:
1. Установил зависимости:
>sudo apt update  
>sudo apt install apt-transport-https ca-certificates curl  
2. Добавил публичный ключ:
>curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg
3. Добавил apt репозиторий:
>echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
4. Обновил индекс apt, установил компоненты k8s:
>sudo apt-get update  
>sudo apt-get install -y kubelet kubeadm kubectl  
>sudo apt-mark hold kubelet kubeadm kubectl  
5. Внес правки в сетевую конфигурацию:  
>modprobe br_netfilter  
>echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf   
>echo "net.bridge.bridge-nf-call-iptables=1" >> /etc/sysctl.conf  
>echo "net.bridge.bridge-nf-call-arptables=1" >> /etc/sysctl.conf  
>echo "net.bridge.bridge-nf-call-ip6tables=1" >> /etc/sysctl.conf  
>sysctl -p /etc/sysctl.conf  
6. Инициализировал мастер ноду:
```
# kubeadm init --apiserver-advertise-address=10.128.0.9 --pod-network-cidr 10.244.0.0/16 --apiserver-cert-extra-sans=*публичный IP*
[init] Using Kubernetes version: v1.27.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0720 16:20:10.185012    4769 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.128.0.9 *публичный IP*]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s localhost] and IPs [10.128.0.9 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s localhost] and IPs [10.128.0.9 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 14.002542 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: lje4zf.z697x00egz83olkj
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join *публичный IP*:6443 --token lje4zf.z697x00egz83olkj \
	--discovery-token-ca-cert-hash sha256:a0f4938ae0ce785e20402e11282bf946afa2b38e609e8f39fbb41baba299f36c \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join *публичный IP*:6443 --token lje4zf.z697x00egz83olkj \
	--discovery-token-ca-cert-hash sha256:a0f4938ae0ce785e20402e11282bf946afa2b38e609e8f39fbb41baba299f36c 

```
7. Инициализировал worker ноды:
>kubeadm join *публичный IP*:6443 --token lje4zf.z697x00egz83olkj \
	--discovery-token-ca-cert-hash sha256:a0f4938ae0ce785e20402e11282bf946afa2b38e609e8f39fbb41baba299f36c 
8. Прописал пользователю конфигурацию для `kubectl`:
```
mkdir -p $HOME/.kube    
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
9. Узнал статус нод:
```
$ kubectl get nodes
NAME     STATUS     ROLES           AGE     VERSION
k8s      NotReady   control-plane   3m18s   v1.27.4
k8s-w1   NotReady   <none>          2m5s    v1.27.4
k8s-w2   NotReady   <none>          2m6s    v1.27.4
k8s-w3   NotReady   <none>          2m6s    v1.27.4
k8s-w4   NotReady   <none>          2m3s    v1.27.4
```
10. Выяснил причину `NotReady`:
```
$ kubectl describe node k8s
Name:               k8s
Roles:              control-plane
...
16:20:39 +0000   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
...
```
11. Так как на лекции не стали заострять внимание на этом плагине, взял первый из гугла:
```
$ kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created
```
12. Повторно проверил статус нод:
```
$ kubectl get nodes
NAME     STATUS   ROLES           AGE     VERSION
k8s      Ready    control-plane   4m56s   v1.27.4
k8s-w1   Ready    <none>          3m43s   v1.27.4
k8s-w2   Ready    <none>          3m44s   v1.27.4
k8s-w3   Ready    <none>          3m44s   v1.27.4
k8s-w4   Ready    <none>          3m41s   v1.27.4
```
### Правила приёма работы

1. Домашняя работа оформляется в своем Git-репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Файл README.md должен содержать скриншоты вывода необходимых команд `kubectl get nodes`, а также скриншоты результатов.
3. Репозиторий должен содержать тексты манифестов или ссылки на них в файле README.md.
